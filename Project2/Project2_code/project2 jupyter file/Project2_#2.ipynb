{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suitable-weekly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist data shape :  (70000, 784)\n",
      "mnist target shape :  (70000,)\n"
     ]
    }
   ],
   "source": [
    "# 2. (Local Descriptor) Based on the descriptor designed in FL class, design the followings.\n",
    "import sklearn.datasets as dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "# data load\n",
    "mnist = dataset.fetch_openml('mnist_784')\n",
    "mnist_data = mnist.data.to_numpy()\n",
    "mnist_target = mnist.target.to_numpy()\n",
    "print('mnist data shape : ',mnist_data.shape)\n",
    "print('mnist target shape : ',mnist_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "final-light",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset length:  30000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nprint('\\nSeperation')\\nprint('training length : ', len(training_data))\\nprint('test_length     : ',len(test_data))\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select(data, target, number=20000):\n",
    "    subset_idx = np.sort( np.random.choice(len(data), number,replace=False) )\n",
    "    \n",
    "    subset_data = data[subset_idx]\n",
    "    subset_target = target[subset_idx]\n",
    "    return subset_data, subset_target\n",
    "\n",
    "subset_data, subset_target  = select(mnist_data, mnist_target, number=30000)\n",
    "\n",
    "'''\n",
    "def separate(data,target,testset_rate=0.2):\n",
    "    rate = testset_rate\n",
    "\n",
    "    testset_idx = np.sort( np.random.choice(len(data), round( len(data)* rate),replace=False) )\n",
    "    trainingset_idx = np.sort( np.setdiff1d(np.arange(len(data)), testset_idx) )\n",
    "\n",
    "    test_data = data[testset_idx]\n",
    "    test_target = target[testset_idx]\n",
    "\n",
    "    training_data = data[trainingset_idx]\n",
    "    training_target = target[trainingset_idx]\n",
    "\n",
    "    return test_data, test_target, training_data,training_target\n",
    "\n",
    " test_data, test_target, training_data,training_target =  separate( subset_data, subset_target,testset_rate=0.1)\n",
    "'''\n",
    "\n",
    "print('subset length: ', len(subset_data))\n",
    "\n",
    "'''\n",
    "print('\\nSeperation')\n",
    "print('training length : ', len(training_data))\n",
    "print('test_length     : ',len(test_data))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "miniature-aggregate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784)\n"
     ]
    }
   ],
   "source": [
    "initial_centroid = []\n",
    "initial_centroid.append(subset_data[np.where(subset_target=='0')[0][3]])\n",
    "initial_centroid.append(subset_data[np.where(subset_target=='1')[0][3]])\n",
    "initial_centroid.append(subset_data[np.where(subset_target=='2')[0][3]])\n",
    "initial_centroid.append(subset_data[np.where(subset_target=='3')[0][3]])\n",
    "initial_centroid.append(subset_data[np.where(subset_target=='4')[0][3]])\n",
    "initial_centroid.append(subset_data[np.where(subset_target=='5')[0][3]])\n",
    "initial_centroid.append(subset_data[np.where(subset_target=='6')[0][3]])\n",
    "initial_centroid.append(subset_data[np.where(subset_target=='7')[0][3]])\n",
    "initial_centroid.append(subset_data[np.where(subset_target=='8')[0][3]])\n",
    "initial_centroid.append(subset_data[np.where(subset_target=='9')[0][3]])\n",
    "initial_centroid = np.array(initial_centroid)\n",
    "print(initial_centroid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "appointed-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Apply k-means (k=10) algorithm to MNIST test dataset, where input dimension is 28^2=784\n",
    "\n",
    "k =10 \n",
    "def kmeans(data, k, feature_size = 784, feature_scale = 256, centroid=None):\n",
    "    data = data.copy()\n",
    "    repeat = 1 \n",
    "    if centroid == None:\n",
    "        centroid = np.random.uniform(0,feature_scale,(k,feature_size)) # random initialization\n",
    "    print('start')\n",
    "    \n",
    "    while True:\n",
    "        repeat+=1\n",
    "        \n",
    "        # groups, ndexes\n",
    "        groups, indexes = [], []\n",
    "    \n",
    "        # cluster 개수에 맞게 리스트 추가\n",
    "        for d in range(k):\n",
    "            groups.append([])\n",
    "            indexes.append([])\n",
    "            \n",
    "        # 거리에 따라 group에 각 feature 값과 index 추가\n",
    "        for i in range(len(data)):\n",
    "            distances = np.array([])\n",
    "            for d in range(k):\n",
    "                distances = np.append(distances, np.sum((data[i] - centroid[d])**2))\n",
    "            idx=np.argmin(distances)\n",
    "            groups[idx].append(data[i])\n",
    "            indexes[idx].append(i)\n",
    "\n",
    "        # 각 그룹의 feature 평균값으로 다음 centroid 계산\n",
    "        next_centroid = np.zeros( (k,feature_size) )\n",
    "        for d in range(k):\n",
    "            if len(indexes[d])!=0:\n",
    "                next_centroid[d] = np.mean(np.array(groups[d]),axis=0)\n",
    "        if repeat % 20 == 0:\n",
    "            print('repeat:', repeat)\n",
    "        \n",
    "        # 종료조건\n",
    "        if(np.array_equal(centroid, next_centroid)):\n",
    "            print('find!')\n",
    "            break\n",
    "        centroid = next_centroid\n",
    "    \n",
    "    print('repeat:', repeat)\n",
    "    return centroid, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "integral-charm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "repeat: 20\n",
      "repeat: 40\n",
      "repeat: 60\n",
      "repeat: 80\n",
      "repeat: 100\n",
      "find!\n",
      "repeat: 100\n"
     ]
    }
   ],
   "source": [
    "centroid, group_indexes = kmeans(subset_data, 10, 784, 256)#, initial_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "valued-selection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 7 3 6 8 4 2 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# centriod에 target 할당\n",
    "# print(training_target[group_indexes[0]])\n",
    "cluster_target = []\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[0]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[1]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[2]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[3]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[4]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[5]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[6]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[7]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[8]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[9]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "print(np.array(cluster_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "grateful-thesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 59.01 %\n"
     ]
    }
   ],
   "source": [
    "# 2) Measure the clustering accuracy by counting the number of incorrectly grouped images, and analyze the results. \n",
    "count=0\n",
    "for i in range(10):\n",
    "    count += len(np.where(subset_target[group_indexes[i]] == str(cluster_target[i]) )[0])\n",
    "print('accuracy:',100 * (count / len(subset_data)), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "purple-catering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncorrect = 0\\nprint(len(test_data))\\n\\nfor i in range(len(test_data)):\\n    distances = np.array([])\\n    for d in range(10):\\n        distances = np.append(distances, np.sum((test_data[i] - centriod[d])**2))\\n    idx=np.argmin(distances)\\n    target = cluster_target[idx]\\n    if int(test_target[i]) == target:\\n        correct+=1\\n\\nprint('test_data_lenght:', len(test_data))\\nprint('accuracy: ', 100*(correct/len(test_data)),'%')\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "correct = 0\n",
    "print(len(test_data))\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    distances = np.array([])\n",
    "    for d in range(10):\n",
    "        distances = np.append(distances, np.sum((test_data[i] - centriod[d])**2))\n",
    "    idx=np.argmin(distances)\n",
    "    target = cluster_target[idx]\n",
    "    if int(test_target[i]) == target:\n",
    "        correct+=1\n",
    "\n",
    "print('test_data_lenght:', len(test_data))\n",
    "print('accuracy: ', 100*(correct/len(test_data)),'%')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cross-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Divide the input 28×28 image into four 14×14 sub-blocks, and compute the histogram of orientations for each sub-block as below\n",
    "\n",
    "def devide(image):\n",
    "    subMatrix = []\n",
    "    subMatrix.append(image[0:14,14:28])\n",
    "    subMatrix.append(image[14:28,0:14])\n",
    "    subMatrix.append(image[0:14,0:14])\n",
    "    subMatrix.append(image[14:28,14:28])\n",
    "    subMatrix = np.array(subMatrix)\n",
    "    return subMatrix\n",
    "\n",
    "def filtering(img, kernel):   # convolution\n",
    "    h, w = img.shape\n",
    "    fSize = len(kernel)       # filter Size\n",
    "    pSize = (fSize-1) // 2    # padding Size\n",
    "    \n",
    "    img = np.pad(img, ((pSize,pSize),(pSize,pSize)), 'symmetric')\n",
    "    filteredImg = np.zeros((h,w))\n",
    "    \n",
    "    for i in range(pSize,h+pSize):      # operate on ground-truth pixel\n",
    "        for j in range(pSize,w+pSize):\n",
    "            product = img[i-pSize:i+pSize+1,j-pSize:j+pSize+1] * kernel\n",
    "            filteredImg[i-pSize][j-pSize] = product.sum()\n",
    "            \n",
    "    return filteredImg\n",
    "\n",
    "def gaussianFilter(size, std):\n",
    "    kernel = np.ones((size,size))\n",
    "    filterOffset = size // 2\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            kernel[i][j] = math.exp( -1 * ( (i-filterOffset)**2 + (j-filterOffset)**2 ) / (2*(std**2)) )\n",
    "    return kernel / kernel.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "medium-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orientation_histogram(img):\n",
    "    #Gaussian filtering \n",
    "    Gaussian_kernel = gaussianFilter(3,3)\n",
    "    img = filtering(img, Gaussian_kernel)\n",
    "    \n",
    "    # magnitude, direction Matrix 생성\n",
    "    h, w = img.shape\n",
    "    magnitude = np.zeros((h,w))\n",
    "    direction = np.zeros((h,w))\n",
    "    img = np.pad(img, ((1,1),(1,1)), 'symmetric')\n",
    "    for i in range(h):  # y\n",
    "        for j in range(w): #x\n",
    "            temp_y = i+1\n",
    "            temp_x = j+1\n",
    "            magnitude[i,j] = np.sqrt((img[ temp_y, temp_x+1] - img[temp_y, temp_x-1])**2 + \\\n",
    "                                                                                (img[temp_y+1,temp_x] - img[temp_y-1,temp_x])**2)\n",
    "            direction[i,j] = np.rad2deg(np.arctan2(img[temp_y+1,temp_x] - img[temp_y-1,temp_x], \\\n",
    "                                                                                img[temp_y, temp_x+1] - img[temp_y, temp_x-1] ))\n",
    "            \n",
    "    # histogram 생성\n",
    "    bins =8\n",
    "    width = np.rad2deg((np.pi*2 ) / bins)           # 45도\n",
    "    histogram = np.zeros(bins) \n",
    "    temp_mag = magnitude.ravel()                    # 반복문 사용 편리를 위해 펼침\n",
    "    direction = direction.ravel()         \n",
    "    direction[ direction < 0 ] += 360               # -180 ~ 180 -> 0 ~ 360으로 변환\n",
    "    hist_index = (direction // width).astype(int)   # 각 direction값을 45로 나눈 몫을 histogram index로 사용\n",
    "    hist_index[hist_index==8] = 0                   # 360도 == 0도\n",
    "    for i in range( len(hist_index) ):\n",
    "        histogram[hist_index[i]] += temp_mag[i]     # magnitude 누적\n",
    "    \n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "worthy-engagement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntest_feature = []\\nfor i in range(len(test_data)):\\n    temp = []\\n    subMatrix = devide(test_data[i].reshape(28,28))\\n    for j in range(len(subMatrix)):\\n        temp.append(orientation_histogram(subMatrix[j]))\\n    temp = np.array(temp).ravel()\\n    test_feature.append(temp)\\ntest_feature  = np.array(test_feature)\\nprint('test_feature shape: ', test_feature.shape )   \\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature Extraction\n",
    "subset_feature = []\n",
    "for i in range(len(subset_data)):\n",
    "    temp = []\n",
    "    subMatrix = devide(subset_data[i].reshape(28,28))\n",
    "    for j in range(len(subMatrix)):\n",
    "        temp.append(orientation_histogram(subMatrix[j]))\n",
    "    temp = np.array(temp).ravel()\n",
    "    subset_feature.append(temp)\n",
    "subset_feature  = np.array(subset_feature)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "test_feature = []\n",
    "for i in range(len(test_data)):\n",
    "    temp = []\n",
    "    subMatrix = devide(test_data[i].reshape(28,28))\n",
    "    for j in range(len(subMatrix)):\n",
    "        temp.append(orientation_histogram(subMatrix[j]))\n",
    "    temp = np.array(temp).ravel()\n",
    "    test_feature.append(temp)\n",
    "test_feature  = np.array(test_feature)\n",
    "print('test_feature shape: ', test_feature.shape )   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "going-appliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset_feature shape:  (30000, 32)\n"
     ]
    }
   ],
   "source": [
    "print('subset_feature shape: ', subset_feature.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "early-burke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "repeat: 20\n",
      "repeat: 40\n",
      "find!\n",
      "repeat: 42\n"
     ]
    }
   ],
   "source": [
    "# 4) Apply k-means (k=10) algorithm again using feature, where input dimension is 8×4=32.\n",
    "\n",
    "centriod, group_indexes = kmeans(subset_feature, 10, feature_size = 32, feature_scale = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "sized-ordinary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 3, 8, 9, 5, 7, 1, 4, 2]\n",
      "(10, 32)\n"
     ]
    }
   ],
   "source": [
    "cluster_target = []\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[0]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[1]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[2]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[3]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[4]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[5]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[6]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[7]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[8]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "cluster_target.append(np.argmax(np.histogram((subset_target[group_indexes[9]]).astype(int),bins=[0, 1, 2, 3,4,5,6,7,8,9,10])[0]) )\n",
    "print(cluster_target)\n",
    "print(centriod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "basic-latvia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 57.43666666666667 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ncorrect = 0\\nprint(len(subset_feature))\\n\\nfor i in range(len(subset_feature)):\\n    distances = np.array([])\\n    for d in range(10):\\n        distances = np.append(distances, np.linalg.norm(subset_feature[i]-centriod[d]))\\n    idx=np.argmin(distances)\\n    target = cluster_target[idx]\\n    if int(subset_target[i]) == target:\\n        correct+=1\\n\\nprint('test_data_lenght:', len(subset_data))\\nprint('accuracy: ', 100*(correct/len(subset_data)),'%')\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) Measure the clustering accuracy for feature-based approach, and analyze the results.\n",
    "\n",
    "count=0\n",
    "for i in range(10):\n",
    "    count += len(np.where(subset_target[group_indexes[i]] == str(cluster_target[i]) )[0])\n",
    "print('accuracy:',100 * (count / len(subset_data)), '%')\n",
    "\n",
    "'''\n",
    "correct = 0\n",
    "print(len(subset_feature))\n",
    "\n",
    "for i in range(len(subset_feature)):\n",
    "    distances = np.array([])\n",
    "    for d in range(10):\n",
    "        distances = np.append(distances, np.linalg.norm(subset_feature[i]-centriod[d]))\n",
    "    idx=np.argmin(distances)\n",
    "    target = cluster_target[idx]\n",
    "    if int(subset_target[i]) == target:\n",
    "        correct+=1\n",
    "\n",
    "print('test_data_lenght:', len(subset_data))\n",
    "print('accuracy: ', 100*(correct/len(subset_data)),'%')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-thomas",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
